{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion_Detection with live cameraipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZxvT3JzzsR_",
        "colab_type": "text"
      },
      "source": [
        "# In this project we are analyising the facial expression using keras (DEEP LEARNING) library. \n",
        "# In this project we are using online dataset for train our model. [Here is the link of dataset](https://www.kaggle.com/ahmedmoorsy/facial-expression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqX058Guut-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import files\n",
        "#upload=files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN1sqLpV1bqR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b06862c0-3594-4bdb-cf75-a40a0148badb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM-x3pJ7vs9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVnrM3Gxv06D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "122a2672-b66c-4f2d-90fd-876cffdd75e5"
      },
      "source": [
        "# pd.set_option('display.max_rows', 500)\n",
        "# pd.set_option('display.max_columns', 500)\n",
        "# pd.set_option('display.width', 1000)\n",
        "\n",
        "df=pd.read_csv('/content/gdrive/My Drive/fer2013.csv')\n",
        "\n",
        "# print(df.info())\n",
        "# print(df[\"Usage\"].value_counts())\n",
        "\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   emotion                                             pixels     Usage\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIMIBqaNwEay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,train_y,X_test,test_y=[],[],[],[]\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    val=row['pixels'].split(\" \")\n",
        "    try:\n",
        "        if 'Training' in row['Usage']:\n",
        "           X_train.append(np.array(val,'float32'))\n",
        "           train_y.append(row['emotion'])\n",
        "        elif 'PublicTest' in row['Usage']:\n",
        "           X_test.append(np.array(val,'float32'))\n",
        "           test_y.append(row['emotion'])\n",
        "    except:\n",
        "        print(f\"error occured at index :{index} and row:{row}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSS5RU8pwOLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = 64\n",
        "num_labels = 7\n",
        "batch_size = 64\n",
        "epochs =50\n",
        "width, height = 48, 48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbu8S0NVwW_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.array(X_train,'float32')\n",
        "train_y = np.array(train_y,'float32')\n",
        "X_test = np.array(X_test,'float32')\n",
        "test_y = np.array(test_y,'float32')\n",
        "\n",
        "train_y=np_utils.to_categorical(train_y, num_classes=num_labels)\n",
        "test_y=np_utils.to_categorical(test_y, num_classes=num_labels)\n",
        "\n",
        "#cannot produce\n",
        "#normalizing data between oand 1\n",
        "X_train -= np.mean(X_train, axis=0)\n",
        "X_train /= np.std(X_train, axis=0)\n",
        "\n",
        "X_test -= np.mean(X_test, axis=0)\n",
        "X_test /= np.std(X_test, axis=0)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
        "\n",
        "# print(f\"shape:{X_train.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbrkxkK1LvY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e01eef32-ffaa-43e9-ebc7-0d3df9de4218"
      },
      "source": [
        "##designing the cnn\n",
        "#1st convolution layer\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))\n",
        "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#2nd convolution layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#3rd convolution layer\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "#fully connected neural networks\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(num_labels, activation='softmax'))\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "#Compliling the model\n",
        "model.compile(loss=categorical_crossentropy,\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Training the model\n",
        "\n",
        "path_model='model_filter.h5'\n",
        "\n",
        "model.fit(X_train, train_y,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, test_y),\n",
        "          shuffle=True,\n",
        "          callbacks=[ModelCheckpoint(filepath=path_model),])\n",
        "\n",
        "\n",
        "#Saving the  model to  use it later on\n",
        "#fer_json = model.to_json()\n",
        "#with open(\"fer.json\", \"w\") as json_file:\n",
        " #   json_file.write(fer_json)\n",
        "#model.save_weights(\"fer.h5\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/50\n",
            "28709/28709 [==============================] - 23s 787us/step - loss: 1.7159 - accuracy: 0.2985 - val_loss: 1.5377 - val_accuracy: 0.3993\n",
            "Epoch 2/50\n",
            "28709/28709 [==============================] - 22s 762us/step - loss: 1.4937 - accuracy: 0.4140 - val_loss: 1.3974 - val_accuracy: 0.4525\n",
            "Epoch 3/50\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 1.3909 - accuracy: 0.4595 - val_loss: 1.3053 - val_accuracy: 0.4990\n",
            "Epoch 4/50\n",
            "28709/28709 [==============================] - 22s 766us/step - loss: 1.3238 - accuracy: 0.4877 - val_loss: 1.2742 - val_accuracy: 0.5049\n",
            "Epoch 5/50\n",
            "28709/28709 [==============================] - 22s 763us/step - loss: 1.2873 - accuracy: 0.5052 - val_loss: 1.2666 - val_accuracy: 0.5040\n",
            "Epoch 6/50\n",
            "28709/28709 [==============================] - 22s 775us/step - loss: 1.2524 - accuracy: 0.5154 - val_loss: 1.2543 - val_accuracy: 0.5116\n",
            "Epoch 7/50\n",
            "28709/28709 [==============================] - 22s 764us/step - loss: 1.2184 - accuracy: 0.5292 - val_loss: 1.1936 - val_accuracy: 0.5447\n",
            "Epoch 8/50\n",
            "28709/28709 [==============================] - 22s 764us/step - loss: 1.1876 - accuracy: 0.5446 - val_loss: 1.1862 - val_accuracy: 0.5378\n",
            "Epoch 9/50\n",
            "28709/28709 [==============================] - 22s 766us/step - loss: 1.1675 - accuracy: 0.5535 - val_loss: 1.1672 - val_accuracy: 0.5483\n",
            "Epoch 10/50\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 1.1453 - accuracy: 0.5636 - val_loss: 1.1602 - val_accuracy: 0.5603\n",
            "Epoch 11/50\n",
            "28709/28709 [==============================] - 22s 765us/step - loss: 1.1197 - accuracy: 0.5685 - val_loss: 1.1562 - val_accuracy: 0.5598\n",
            "Epoch 12/50\n",
            "28709/28709 [==============================] - 22s 761us/step - loss: 1.1007 - accuracy: 0.5798 - val_loss: 1.1415 - val_accuracy: 0.5595\n",
            "Epoch 13/50\n",
            "28709/28709 [==============================] - 22s 763us/step - loss: 1.0792 - accuracy: 0.5878 - val_loss: 1.1539 - val_accuracy: 0.5564\n",
            "Epoch 14/50\n",
            "28709/28709 [==============================] - 22s 766us/step - loss: 1.0685 - accuracy: 0.5905 - val_loss: 1.1478 - val_accuracy: 0.5642\n",
            "Epoch 15/50\n",
            "28709/28709 [==============================] - 22s 763us/step - loss: 1.0471 - accuracy: 0.5987 - val_loss: 1.1714 - val_accuracy: 0.5587\n",
            "Epoch 16/50\n",
            "28709/28709 [==============================] - 22s 768us/step - loss: 1.0267 - accuracy: 0.6066 - val_loss: 1.1502 - val_accuracy: 0.5731\n",
            "Epoch 17/50\n",
            "28709/28709 [==============================] - 22s 765us/step - loss: 1.0146 - accuracy: 0.6117 - val_loss: 1.1489 - val_accuracy: 0.5662\n",
            "Epoch 18/50\n",
            "28709/28709 [==============================] - 22s 761us/step - loss: 0.9995 - accuracy: 0.6174 - val_loss: 1.1599 - val_accuracy: 0.5620\n",
            "Epoch 19/50\n",
            "28709/28709 [==============================] - 22s 766us/step - loss: 0.9801 - accuracy: 0.6280 - val_loss: 1.1739 - val_accuracy: 0.5751\n",
            "Epoch 20/50\n",
            "28709/28709 [==============================] - 22s 770us/step - loss: 0.9671 - accuracy: 0.6289 - val_loss: 1.1684 - val_accuracy: 0.5706\n",
            "Epoch 21/50\n",
            "28709/28709 [==============================] - 22s 762us/step - loss: 0.9516 - accuracy: 0.6380 - val_loss: 1.1691 - val_accuracy: 0.5743\n",
            "Epoch 22/50\n",
            "28709/28709 [==============================] - 22s 762us/step - loss: 0.9386 - accuracy: 0.6407 - val_loss: 1.1542 - val_accuracy: 0.5807\n",
            "Epoch 23/50\n",
            "28709/28709 [==============================] - 22s 767us/step - loss: 0.9175 - accuracy: 0.6499 - val_loss: 1.1839 - val_accuracy: 0.5795\n",
            "Epoch 24/50\n",
            "28709/28709 [==============================] - 22s 759us/step - loss: 0.9076 - accuracy: 0.6557 - val_loss: 1.1585 - val_accuracy: 0.5720\n",
            "Epoch 25/50\n",
            "28709/28709 [==============================] - 22s 759us/step - loss: 0.8866 - accuracy: 0.6649 - val_loss: 1.1727 - val_accuracy: 0.5648\n",
            "Epoch 26/50\n",
            "28709/28709 [==============================] - 22s 760us/step - loss: 0.8765 - accuracy: 0.6675 - val_loss: 1.1888 - val_accuracy: 0.5726\n",
            "Epoch 27/50\n",
            "28709/28709 [==============================] - 22s 760us/step - loss: 0.8624 - accuracy: 0.6721 - val_loss: 1.1812 - val_accuracy: 0.5726\n",
            "Epoch 28/50\n",
            "28709/28709 [==============================] - 22s 760us/step - loss: 0.8467 - accuracy: 0.6756 - val_loss: 1.1900 - val_accuracy: 0.5665\n",
            "Epoch 29/50\n",
            "28709/28709 [==============================] - 22s 762us/step - loss: 0.8299 - accuracy: 0.6867 - val_loss: 1.2052 - val_accuracy: 0.5787\n",
            "Epoch 30/50\n",
            "28709/28709 [==============================] - 22s 762us/step - loss: 0.8215 - accuracy: 0.6878 - val_loss: 1.2107 - val_accuracy: 0.5709\n",
            "Epoch 31/50\n",
            "28709/28709 [==============================] - 22s 757us/step - loss: 0.8088 - accuracy: 0.6916 - val_loss: 1.2567 - val_accuracy: 0.5653\n",
            "Epoch 32/50\n",
            "28709/28709 [==============================] - 22s 756us/step - loss: 0.7934 - accuracy: 0.6999 - val_loss: 1.2401 - val_accuracy: 0.5807\n",
            "Epoch 33/50\n",
            "28709/28709 [==============================] - 22s 758us/step - loss: 0.7846 - accuracy: 0.7014 - val_loss: 1.2370 - val_accuracy: 0.5770\n",
            "Epoch 34/50\n",
            "28709/28709 [==============================] - 22s 766us/step - loss: 0.7692 - accuracy: 0.7110 - val_loss: 1.1899 - val_accuracy: 0.5779\n",
            "Epoch 35/50\n",
            "28709/28709 [==============================] - 22s 766us/step - loss: 0.7584 - accuracy: 0.7137 - val_loss: 1.2423 - val_accuracy: 0.5812\n",
            "Epoch 36/50\n",
            "28709/28709 [==============================] - 22s 761us/step - loss: 0.7539 - accuracy: 0.7146 - val_loss: 1.2645 - val_accuracy: 0.5715\n",
            "Epoch 37/50\n",
            "28709/28709 [==============================] - 22s 765us/step - loss: 0.7410 - accuracy: 0.7214 - val_loss: 1.2688 - val_accuracy: 0.5812\n",
            "Epoch 38/50\n",
            "28709/28709 [==============================] - 22s 762us/step - loss: 0.7266 - accuracy: 0.7242 - val_loss: 1.2860 - val_accuracy: 0.5807\n",
            "Epoch 39/50\n",
            "28709/28709 [==============================] - 22s 761us/step - loss: 0.7107 - accuracy: 0.7316 - val_loss: 1.3015 - val_accuracy: 0.5759\n",
            "Epoch 40/50\n",
            "28709/28709 [==============================] - 22s 760us/step - loss: 0.6996 - accuracy: 0.7365 - val_loss: 1.3018 - val_accuracy: 0.5765\n",
            "Epoch 41/50\n",
            "28709/28709 [==============================] - 22s 763us/step - loss: 0.7026 - accuracy: 0.7340 - val_loss: 1.3205 - val_accuracy: 0.5715\n",
            "Epoch 42/50\n",
            "28709/28709 [==============================] - 22s 757us/step - loss: 0.6967 - accuracy: 0.7397 - val_loss: 1.2478 - val_accuracy: 0.5798\n",
            "Epoch 43/50\n",
            "28709/28709 [==============================] - 22s 763us/step - loss: 0.6812 - accuracy: 0.7424 - val_loss: 1.2797 - val_accuracy: 0.5809\n",
            "Epoch 44/50\n",
            "28709/28709 [==============================] - 22s 760us/step - loss: 0.6682 - accuracy: 0.7495 - val_loss: 1.3188 - val_accuracy: 0.5787\n",
            "Epoch 45/50\n",
            "28709/28709 [==============================] - 22s 759us/step - loss: 0.6656 - accuracy: 0.7537 - val_loss: 1.3452 - val_accuracy: 0.5748\n",
            "Epoch 46/50\n",
            "28709/28709 [==============================] - 22s 760us/step - loss: 0.6438 - accuracy: 0.7560 - val_loss: 1.3778 - val_accuracy: 0.5748\n",
            "Epoch 47/50\n",
            "28709/28709 [==============================] - 22s 752us/step - loss: 0.6530 - accuracy: 0.7577 - val_loss: 1.3501 - val_accuracy: 0.5670\n",
            "Epoch 48/50\n",
            "28709/28709 [==============================] - 22s 763us/step - loss: 0.6394 - accuracy: 0.7622 - val_loss: 1.3517 - val_accuracy: 0.5874\n",
            "Epoch 49/50\n",
            "28709/28709 [==============================] - 22s 763us/step - loss: 0.6304 - accuracy: 0.7661 - val_loss: 1.3481 - val_accuracy: 0.5854\n",
            "Epoch 50/50\n",
            "28709/28709 [==============================] - 22s 756us/step - loss: 0.6169 - accuracy: 0.7728 - val_loss: 1.3886 - val_accuracy: 0.5874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f232b3821d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT2Tk2RuzAjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "from time import sleep\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing import image\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3M2BhZwB8oC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "face_classifier = cv2.CascadeClassifier('/content/gdrive/My Drive/haarcascade_frontalface_default.xml')\n",
        "classifier =load_model('/content/model_filter.h5')\n",
        "\n",
        "class_labels = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    # Grab a single frame of video\n",
        "    ret, frame = cap.read()\n",
        "    labels = []\n",
        "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_classifier.detectMultiScale(gray,1.3,5)\n",
        "\n",
        "    for (x,y,w,h) in faces:\n",
        "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "        roi_gray = gray[y:y+h,x:x+w]\n",
        "        roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
        "    # rect,face,image = face_detector(frame)\n",
        "\n",
        "\n",
        "        if np.sum([roi_gray])!=0:\n",
        "            roi = roi_gray.astype('float')/255.0\n",
        "            roi = img_to_array(roi)\n",
        "            roi = np.expand_dims(roi,axis=0)\n",
        "\n",
        "        # make a prediction on the ROI, then lookup the class\n",
        "\n",
        "            preds = classifier.predict(roi)[0]\n",
        "            label=class_labels[preds.argmax()]\n",
        "            label_position = (x,y)\n",
        "            cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
        "        else:\n",
        "            cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
        "    cv2.imshow('Emotion Detector',frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6wx6TzyB9BC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
